{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('',train = True, download = True\n",
    "                      , transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "test = datasets.MNIST('',train = False, download = True\n",
    "                      , transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10,shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #your OOP\n",
    "import torch.nn.functional as F #the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #run the initialization for nn.Module, same as nn.Module.__init__() \n",
    "        self.fc1 = nn.Linear(28*28, 64) #fully connected linear layer, format (input_nrs, output_nrs) \n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) #use rectified linear function as activation function.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.softmax(x,dim=1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28)).view(-1, 28*28) #view is same as reshape in numpy, -1 specifies input will be of unknown shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1040, 0.1008, 0.0974, 0.1009, 0.0937, 0.1119, 0.1047, 0.1028, 0.1004,\n",
       "         0.0833]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([0, 1, 2, 2, 3, 4, 9, 3, 0, 1])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6886, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5612, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5611, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "EPOCHS = 3\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a batch of featuresets and labels.\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        loss_output = loss(output, y)\n",
    "        loss_output.backward()\n",
    "        optimizer.step() #adjusts the weights for us.\n",
    "    print(loss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[4.0058e-08, 2.5769e-10, 3.7100e-07, 9.7223e-06, 7.1895e-01, 1.6745e-03,\n",
      "         8.7231e-10, 2.7936e-01, 4.9627e-14, 1.5596e-12],\n",
      "        [1.1783e-13, 1.4832e-26, 1.0000e+00, 1.3677e-11, 1.4857e-18, 2.0809e-15,\n",
      "         1.9535e-21, 3.5671e-21, 6.6527e-28, 5.9218e-30],\n",
      "        [9.9659e-01, 6.7798e-14, 2.4870e-05, 2.3553e-07, 2.9129e-12, 3.3884e-03,\n",
      "         2.6827e-13, 1.5005e-08, 1.8848e-16, 3.3424e-17],\n",
      "        [3.8807e-11, 1.1131e-01, 1.8805e-06, 3.1084e-02, 2.1091e-01, 5.2157e-01,\n",
      "         1.7207e-08, 1.2513e-01, 6.6050e-13, 3.2164e-13],\n",
      "        [2.3564e-06, 1.2315e-14, 7.8026e-07, 5.2806e-11, 4.5939e-12, 1.0000e+00,\n",
      "         1.9171e-10, 3.5363e-19, 1.1288e-24, 9.2270e-26],\n",
      "        [9.0808e-10, 5.2383e-17, 9.5600e-10, 4.9522e-14, 1.0000e+00, 1.3942e-06,\n",
      "         2.9925e-07, 4.7846e-07, 1.3960e-20, 1.2089e-17],\n",
      "        [3.3216e-13, 1.9180e-05, 1.7189e-05, 5.1952e-04, 3.2696e-08, 2.8263e-05,\n",
      "         1.2973e-15, 9.9942e-01, 6.7988e-18, 4.2526e-18],\n",
      "        [5.0198e-08, 1.0331e-14, 1.0000e+00, 2.1441e-08, 6.4709e-09, 7.2658e-07,\n",
      "         1.8755e-11, 6.7947e-10, 2.4810e-18, 1.5792e-18],\n",
      "        [6.3650e-14, 8.5639e-11, 1.0377e-07, 9.5640e-08, 9.9674e-01, 4.8858e-05,\n",
      "         1.6990e-11, 3.2145e-03, 1.4407e-18, 3.2032e-17],\n",
      "        [1.7333e-14, 9.9999e-01, 4.0002e-06, 9.8694e-07, 1.1473e-06, 4.5785e-06,\n",
      "         3.6291e-07, 1.6918e-06, 1.6722e-14, 4.1659e-15]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([9, 2, 0, 9, 5, 4, 7, 2, 9, 1])\n"
     ]
    }
   ],
   "source": [
    "F.nll_loss(output, y)\n",
    "print(X.view(-1,28*28))\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.954\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total +=1\n",
    "            \n",
    "print(\"Accuracy: \", round(correct/total,3))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = 2\n",
    "plt.imshow(X[index].view(28,28))\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted: \", torch.argmax(net(X[index].view(-1,28*28))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
